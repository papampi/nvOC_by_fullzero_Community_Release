#!/bin/bash

##########################################################################
##########################################################################
#################   nvOC v0019-2.1 - Community Release   #################
##########   Based on the original nvOC v0019-1.4 by fullzero   ##########
##########################################################################
##########################################################################

# TEMP_CONTROL for nvOC v0019-2.0 by leenoox
#
# Based on the original code by Maxximus007
#
# Changelog:
# v=0001 : leenoox
#          Set higher process and disk I/O priorities - Temp control is essential service
#          Auto detection of number of GPU's
#          Only set, adjust and display available GPU's, dynamic variables creation
#          Numeric check of the return values from nvidia-smi (looking for numbers only, anything else = error)
#          Reboot if GPU error detected and watchdog didn't react for 60 seconds (act as a backup watchdog)
#          Added query delay to nvidia API (no burst spamming, added 0.5 sec delay, prevent overload), helps reduce stale shares
#          New (improved) display output including colors and bold text
#          Fixed the log file handling (Bug fix: Previous implemantation was not limiting the file size)
#          Removed repetitive GPUFanControlState setting, it only needs to be set once, not every cycle (prevent API overload)
#          Workaround for some 1050's reporting "Unknown" or "ERR" when power.draw is queried from nvidi-smi
# v=0002 : Stubo: Added secondary fix for 1050's reporting "[Not Supported]" or "[Unknown Error]" when power.draw is
#          queried from nvidia-smi (a.k.a. bleed issue of power.draw)
# v=0003 : Papampi: Telegram alerts
#
# v=0004 : Papampi:
#          MAXIMAL_FAN_SPEED
# v=0005 : LukePicci
#          Relocate nvOC to arbitrary install directory
# v=0006 : papampi (suggestion by abdeldev)
#          Use sysrq reboot, to prevent freeze/hang
# v=0007 : brightskye
#          Add supports for disabled_gpu

source ${NVOC}/1bash
source ${NVOC}/0algo_id
source ${NVOC}/helpers/disabled_gpu

nvOC_temp_Dev="0006"
nvOC_temp_ver="$nvOC_Ver.$nvOC_temp_Dev"   # Do not edit this

export DISPLAY=:0

echo "Temp Control for nvOC $nvOC_Ver"
echo "Version: $nvOC_temp_ver"
echo ""

# Set higher process and disk I/O priorities because we are essential service
sudo renice -n -15 -p $$ && sudo ionice -c2 -n0 -p$$ >/dev/null 2>&1
sleep 1

NVD="nvidia-settings"
SMI="nvidia-smi"

if [[ $USE_COLOR == YES ]]; then
  N='\e[0m'     # Normal
  B='\e[1m'     # Bold
  R='\e[31m'    # Red
  G='\e[32m'    # Green
  C='\e[36m'    # Cyan
  Y='\e[33m'    # Yellow
else
  N=""
  B=""
  R=""
  G=""
  C=""
  Y=""
fi

# Log file handling (check existance, size limitation or creation, show 10 lines if not empty)
LOG_FILE="${NVOC}/6_autotemplog"
if [ -e "$LOG_FILE" ]; then
  LASTLOG=$(tail -n 100 $LOG_FILE)     # Limit the log file, just keep the last 100 entries
  echo "$LASTLOG" > $LOG_FILE
  if [[ $(wc -l <$LOG_FILE) -gt 1 ]]; then
    echo -e "${B}LOG FILE:${N} (Showing the last 10 recorded entries)${R}"
    cat $LOG_FILE | tail -n 10
    echo -e "${N}"
    echo ""
  else
    echo -e "${B}LOG FILE${N} is empty."
    echo ""
    echo ""
  fi
else
  touch $LOG_FILE     # if log file does not exist, create one
  echo -e "New ${B}LOG FILE${N} created."
  echo ""
  echo ""
fi

chmod a+w "${LOG_FILE}"             # allow clearing by www

WD_LOG_FILE="${NVOC}/5_watchdoglog"

# Display version info
echo ""
for i in {16..21} ; do
  echo -en "\e[48;5;${i}m "
done
echo -en "${B}TEMP_CONTROL $nvOC_temp_ver by leenoox${N}"
for i in {21..16} ; do
  echo -en "\e[48;5;${i}m \e[0m"
done
echo ""
echo ""
sleep 1


# Determine the number of available GPU's
GPUS=$(${SMI} -i 0 --query-gpu=count --format=csv,noheader,nounits)

echo -e "Detected: ${B}$GPUS${N} GPU's"
echo ""
count=0


# Dynamic variables creation - assign variables for the available GPU's only
# Set variables for Temp and Power Limit; Enable fan control; Display info
while (( count < GPUS ))
do
  if [ ${DISABLED_GPU_ARRAY[$count]} ]; then
    # skip if disabled
    echo -e "${B}GPU $count:${N} ${R}${B}Disabled${N}"
    (( count++ ))
    continue
  fi
  if [[ $POWERLIMIT_MODE == GPU_SPECIFIC ]]
  then
    POWER_LIMIT[$count]=$(( INDIVIDUAL_POWERLIMIT_$count ))
  elif [[ $POWERLIMIT_MODE == GLOBAL_with_GPU_OFFSET || $POWERLIMIT_MODE == ALGO_SPECIFIC_with_GPU_OFFSET ]]
  then
    POWER_LIMIT[$count]=$(( ${ALGO}_POWERLIMIT_WATTS + INDIVIDUAL_POWERLIMIT_$count ))
  elif [[ $POWERLIMIT_MODE == ALGO_SPECIFIC ]]
  then
    POWER_LIMIT[$count]=$(( ${ALGO}_POWERLIMIT_WATTS ))
  elif [[ $POWERLIMIT_MODE == GLOBAL ]]
  then
    POWER_LIMIT[$count]=$_POWERLIMIT_WATTS
  fi

  if [[ $INDIVIDUAL_TARGET_TEMPS == YES ]]
  then
    TARGET_TEMP[$count]=$(( TARGET_TEMP_$count ))
  elif [[ $INDIVIDUAL_TARGET_TEMPS == NO ]]
  then
    TARGET_TEMP[$count]=$TARGET_TEMP
  fi

  # Info - display assigned values per GPU
  echo -e "${B}GPU $count:${N}  POWER LIMIT: ${B}${POWER_LIMIT[$count]}${N},  TARGET TEMP: ${B}${TARGET_TEMP[$count]}${N}"

  __CTL_ENABLE="${__CTL_ENABLE} -a [gpu:${count}]/GPUFanControlState=1"
  (( count++ ))
done

# Enable fan control
sudo ${NVD} $__CTL_ENABLE >/dev/null 2>&1
sleep 0.1

FAN_ADJUST=$__FAN_ADJUST

# If user sets the fan speed too low in 1bash, override and set it to 30%
if (( MINIMAL_FAN_SPEED < 30 ))
then
  MINIMAL_FAN_SPEED=30
fi

echo ""
# Info - display the Global settings
echo -e "${B}GLOBAL${N}  FAN_ADJUST (%):  ${B}$FAN_ADJUST${N}"
echo -e "${B}GLOBAL${N}  POWER_ADJUST (W):  ${B}$POWER_ADJUST${N}"
echo -e "${B}GLOBAL${N}  ALLOWED_TEMP_DIFF (C):  ${B}$ALLOWED_TEMP_DIFF${N}"
echo -e "${B}GLOBAL${N}  RESTORE_POWER_LIMIT (%):  ${B}$RESTORE_POWER_LIMIT${N}"
echo -e "${B}GLOBAL${N}  MINIMAL_FAN_SPEED (%):  ${B}$MINIMAL_FAN_SPEED${N}"
echo ""



# Setting persistance mode on, will keep settings in between sessions
# fullzero, Papampi, Stubo..., we need to decide if we gonna use Persistane Mode.
# I am using it for a while and had no problems with it.
# If we decide to use it, please move this to the top of 3main:
# sudo nvidia-smi -pm 1


# How often should TEMP_CONTROL check and adjust the fans
# Allowed value between 15 and 30 seconds (IMO, 20 seconds works well)
LOOP_TIMER_SLEEP=20

if (( LOOP_TIMER_SLEEP < 15 ))
then
  LOOP_TIMER_SLEEP=15
elif (( LOOP_TIMER_SLEEP > 30 ))
then
  LOOP_TIMER_SLEEP=30
fi

# Calculating the main timer dependant on the number of GPU's because
# we are adding 0.5 seconds delay between every GPU check so that
# we don't overload nvidia API (previously the API was spammed, especialy on
# systems with 13+ GPU's causing slight delay for the miner. This has reduced stale shares for me)
LOOP_TIMER=$(echo "$LOOP_TIMER_SLEEP - ( $GPUS * 0.5 )" | bc )

# When API returns error message due to frozen/hung GPU, the original Temp Control script
# was breaking with error because it was expecting numeric but received a text value, leaving
# the system without temp control and potential to damage GPU's.
# Adding numtest check to the returned values from nvidia-smi to prevent such occurance
numtest='^[0-9.]+$'

# Time in seconds before we reboot should we detect error and watchdog didn't react
ERR_TIMER=60
ERR_TIMER_BRK=$ERR_TIMER

# The Main Loop
while true
do
  GPU=0
  while (( GPU < GPUS ))
  do
    if [ ${DISABLED_GPU_ARRAY[$GPU]} ]; then
      # skip if disabled
      echo -e "${B}GPU $GPU,${N} ${R}${B}Disabled${N}"
      (( GPU++ ))
      continue
    fi
    { IFS=', ' read CURRENT_TEMP CURRENT_FAN PWRLIMIT POWERDRAW; } < <(${SMI} -i $GPU --query-gpu=temperature.gpu,fan.speed,power.limit,power.draw --format=csv,noheader,nounits)

    # Numeric check to avoid script breakage should nvidia-smi return error, also acts as backup watchdog

    # Workaround for 1050's reporting "[Not Supported]" or "[Unknown Error]" when power.draw is queried from nvidia-smi
    if [[ $(${SMI} -i $GPU --query-gpu=name --format=csv,noheader,nounits | grep "1050") ]]
    then
      if ! [[ ( $CURRENT_TEMP =~ $numtest ) && ( $CURRENT_FAN =~ $numtest ) && ( $PWRLIMIT =~ $numtest ) ]]
      then
        # Non numeric value! Problem detected! Give watchdog 60 seconds to react, if not, assume watchdog froze - we will reboot in 60 sec (backup watchdog function)
        while (( ERR_TIMER > 0 ))
        do
          echo -e "${R}${B}WARNING: $(date) - Problem detected! GPU$GPU is not responding. Will give watchdog $ERR_TIMER seconds to react, if not we will reboot!${N}" | tee -a "$LOG_FILE"
          if [[ $TELEGRAM_ALERTS == "YES" ]]
          then
            bash "${NVOC}/telegram" &
          fi
          sleep 15
          { IFS=', ' read CURRENT_TEMP CURRENT_FAN PWRLIMIT; } < <(${SMI} -i $GPU --query-gpu=temperature.gpu,fan.speed,power.limit --format=csv,noheader,nounits)
          if ! [[ ( $CURRENT_TEMP =~ $numtest ) && ( $CURRENT_FAN =~ $numtest ) && ( $PWRLIMIT =~ $numtest ) ]]
          then
            ERR_TIMER=$((ERR_TIMER - 15))
          else
            ERR_TIMER=$ERR_TIMER_BRK
            break
          fi
          if (( ERR_TIMER <= 0 ))
          then
            echo -e "${R}${B}WARNING: $(date) - Problem detected with GPU$GPU. Watchdog didn't react. System will reboot by the TEMP_CONTROL to correct the problem!" | tee -a "$LOG_FILE" "$WD_LOG_FILE"
            if [[ $TELEGRAM_ALERTS == "YES" ]]
            then
              bash "${NVOC}/telegram"
            fi
            sleep 3
            if [[ $SYSRQ_REBOOT == YES ]]
            then
              sudo bash ${NVOC}/sysrq_reboot.sh
            else
              sudo reboot
            fi
          fi
        done
      fi
    else
      if ! [[ ( $CURRENT_TEMP =~ $numtest ) && ( $CURRENT_FAN =~ $numtest ) && ( $POWERDRAW =~ $numtest ) && ( $PWRLIMIT =~ $numtest ) ]]
      then
        # Non numeric value! Problem detected! Give watchdog 60 seconds to react, if not, assume watchdog froze - we will reboot in 60 sec (backup watchdog function)
        while (( ERR_TIMER > 0 ))
        do
          echo -e "${R}${B}WARNING: $(date) - Problem detected! GPU$GPU is not responding. Will give watchdog $ERR_TIMER seconds to react, if not we will reboot!${N}" | tee -a "$LOG_FILE"
          if [[ $TELEGRAM_ALERTS == "YES" ]]
          then
            bash "${NVOC}/telegram" &
          fi
          sleep 15
          { IFS=', ' read CURRENT_TEMP CURRENT_FAN PWRLIMIT POWERDRAW; } < <(${SMI} -i $GPU --query-gpu=temperature.gpu,fan.speed,power.limit,power.draw --format=csv,noheader,nounits)
          if ! [[ ( $CURRENT_TEMP =~ $numtest ) && ( $CURRENT_FAN =~ $numtest ) && ( $POWERDRAW =~ $numtest ) && ( $PWRLIMIT =~ $numtest ) ]]
          then
            ERR_TIMER=$((ERR_TIMER - 15))
          else
            ERR_TIMER=$ERR_TIMER_BRK
            break
          fi
          if (( ERR_TIMER <= 0 ))
          then
            echo -e "${R}${B}WARNING: $(date) - Problem detected with GPU$GPU. Watchdog didn't react. System will reboot by the TEMP_CONTROL to correct the problem!" | tee -a "$LOG_FILE" "$WD_LOG_FILE"
            if [[ $TELEGRAM_ALERTS == "YES" ]]
            then
              bash "${NVOC}/telegram" &
            fi
            sleep 3
            if [[ $SYSRQ_REBOOT == YES ]]
            then
              sudo bash ${NVOC}/sysrq_reboot.sh
            else
              sudo reboot
            fi
          fi
        done
      fi
    fi

    POWERLIMIT=${PWRLIMIT%%.*}
    TEMP_DIFF=$(( TARGET_TEMP[GPU] - CURRENT_TEMP ))
    NEW_FAN_SPEED=$CURRENT_FAN
    HIGH_TEMP_DIFF=$(( ALLOWED_TEMP_DIFF * 2 ))

    echo -e "${B}GPU $GPU${N}, Target temp: ${B}${TARGET_TEMP[${GPU}]}${N}, Current: ${B}$CURRENT_TEMP${N}, Diff: ${B}$TEMP_DIFF${N}, Fan: ${B}$CURRENT_FAN${N}, Power: ${B}$POWERDRAW${N}"

    if (( CURRENT_TEMP > TARGET_TEMP[GPU] ))
    then
      # If temp difference is more than double the allowed, multiply adjustement by 2
      if (( TEMP_DIFF < - HIGH_TEMP_DIFF ))
      then
        FAN_ADJUST_CALCULATED=$(( FAN_ADJUST * 2 ))
      else
        FAN_ADJUST_CALCULATED=$FAN_ADJUST
      fi
      NEW_FAN_SPEED=$(( CURRENT_FAN + FAN_ADJUST_CALCULATED ))
      if (( NEW_FAN_SPEED > MAXIMAL_FAN_SPEED ))
      then
        NEW_FAN_SPEED=$MAXIMAL_FAN_SPEED
        # Fan speed reached MAXIMAL_FAN_SPEED, we have to drop the power limit
        NEW_POWER_LIMIT=$((POWERLIMIT - POWER_ADJUST))
        if [[ $WARN_PL_DROPS == YES ]]
        then
          echo -e "${B}WARNING: GPU $GPU${N}, ${R}$(date) - Adjusting Power Limit for ${B}GPU$GPU${N}${R}. Old Limit: ${B}$POWERLIMIT${N}${R} New Limit: ${B}$NEW_POWER_LIMIT${N}${R} Fan speed: ${B}$NEW_FAN_SPEED${N}" | tee -a "$LOG_FILE"
          if [[ $TELEGRAM_ALERTS == "YES" ]]
          then
            bash "${NVOC}/telegram" &
          fi
        else
          echo -e "${B}INFO: GPU $GPU${N}, ${R}$(date) - Adjusting Power Limit for ${B}GPU$GPU${N}${R}. Old Limit: ${B}$POWERLIMIT${N}${R} New Limit: ${B}$NEW_POWER_LIMIT${N}${R} Fan speed: ${B}$NEW_FAN_SPEED${N}"
        fi
        echo ""
        sudo ${SMI} -i $GPU -pl ${NEW_POWER_LIMIT}
      fi
    else
      # Current temp is lower than target, so we can relax fan speed, and restore original power limit if applicable
      if (( TEMP_DIFF > ALLOWED_TEMP_DIFF ))
      then
        # Faster restore fan speed. This can be far more advanced too
        FAN_ADJUST_CALCULATED=$FAN_ADJUST
        if (( TEMP_DIFF > HIGH_TEMP_DIFF ))
        then
          FAN_ADJUST_CALCULATED=$(( FAN_ADJUST * 2 ))
          if (( TEMP_DIFF > HIGH_TEMP_DIFF * 2 ))
          then
            FAN_ADJUST_CALCULATED=$(( FAN_ADJUST * 4 ))
          fi
        fi
        NEW_FAN_SPEED=$(( CURRENT_FAN - FAN_ADJUST_CALCULATED ))
        # Set to minimal fan speed if calculated is below
        if (( NEW_FAN_SPEED < MINIMAL_FAN_SPEED ))
        then
          NEW_FAN_SPEED=$MINIMAL_FAN_SPEED
        fi
        # Restore original power limit when possible using fan speed
        if (( ${POWER_LIMIT[${GPU}]} != POWERLIMIT ))
        then
          if (( NEW_FAN_SPEED < RESTORE_POWER_LIMIT ))
          then
            NEW_POWER_LIMIT=${POWER_LIMIT[${GPU}]}
            echo -e "${B}GPU$GPU${N}${C}$(date) - Restoring Power Limit for ${N}${B}GPU$GPU${N}. ${C}Old limit: ${N}${B}$POWERLIMIT${N}${C} New limit: ${N}${B}$NEW_POWER_LIMIT${N}${C} Fan speed: ${N}${B}$NEW_FAN_SPEED${N}"
            echo ""
            sudo ${SMI} -i $GPU -pl ${NEW_POWER_LIMIT}
          fi
        fi
      fi
    fi

    if (( NEW_FAN_SPEED != CURRENT_FAN ))
    then
      echo -e "${B}GPU $GPU${N}, ${C}$(date) - Adjusting fan from: ${N}${B}$CURRENT_FAN${N} ${C}to: ${N}${B}$NEW_FAN_SPEED${N} ${C}Temp: ${N}${B}$CURRENT_TEMP${N}"
      echo ""
      sudo ${NVD} -a [fan:${GPU}]/GPUTargetFanSpeed=${NEW_FAN_SPEED} 2>&1 >/dev/null
    fi

    (( GPU++ ))
    sleep 0.5    # 0.5 seconds delay until querying the next GPU
  done
  echo "$(date) - All good, will check again in $LOOP_TIMER seconds"
  echo ""
  echo ""
  echo ""
  sleep $LOOP_TIMER
done
